{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Скачиваем и объединяем тексты стихов из сборников\n",
        "!wget https://raw.githubusercontent.com/pltnv-a-a/nlp-notes/refs/heads/main/Boris_Pasternak.txt\n",
        "!wget https://raw.githubusercontent.com/pltnv-a-a/nlp-notes/refs/heads/main/Lev_Losev.txt\n",
        "!wget https://raw.githubusercontent.com/pltnv-a-a/nlp-notes/refs/heads/main/Evgeny_Baratynsky.txt\n",
        "!wget https://raw.githubusercontent.com/pltnv-a-a/nlp-notes/refs/heads/main/Boris_Ryzhyi.txt\n",
        "!wget https://raw.githubusercontent.com/pltnv-a-a/nlp-notes/refs/heads/main/Bulat_Okudjava.txt\n",
        "!wget https://raw.githubusercontent.com/pltnv-a-a/nlp-notes/refs/heads/main/Demiyan_Bedny.txt\n",
        "!wget https://raw.githubusercontent.com/pltnv-a-a/nlp-notes/refs/heads/main/Michail_Ancharov.txt\n",
        "!wget https://raw.githubusercontent.com/pltnv-a-a/nlp-notes/refs/heads/main/Nikolay_Gumilev.txt\n",
        "!wget https://raw.githubusercontent.com/pltnv-a-a/nlp-notes/refs/heads/main/Osip_Mandelshtam.txt\n",
        "!wget https://raw.githubusercontent.com/pltnv-a-a/nlp-notes/refs/heads/main/Vladimir_Nabokov.txt\n",
        "\n",
        "\n",
        "with open('Bulat_Okudjava.txt', 'r', encoding='utf-8') as f:\n",
        "    okudjava = f.read()\n",
        "\n",
        "with open('Demiyan_Bedny.txt', 'r', encoding='utf-8') as f:\n",
        "    bedny = f.read()\n",
        "\n",
        "with open('Michail_Ancharov.txt', 'r', encoding='utf-8') as f:\n",
        "    ancharov = f.read()\n",
        "\n",
        "with open('Nikolay_Gumilev.txt', 'r', encoding='utf-8') as f:\n",
        "    gumilev = f.read()\n",
        "\n",
        "with open('Osip_Mandelshtam.txt', 'r', encoding='utf-8') as f:\n",
        "    mandelshtam = f.read()\n",
        "\n",
        "with open('Vladimir_Nabokov.txt', 'r', encoding='utf-8') as f:\n",
        "    nabokov = f.read()\n",
        "\n",
        "with open('Boris_Pasternak.txt', 'r', encoding='utf-8') as f:\n",
        "    pasternak = f.read()\n",
        "\n",
        "with open('Lev_Losev.txt', 'r', encoding='utf-8') as f:\n",
        "    losev = f.read()\n",
        "\n",
        "with open('Evgeny_Baratynsky.txt', 'r', encoding='utf-8') as f:\n",
        "    bar = f.read()\n",
        "\n",
        "with open('Boris_Ryzhyi.txt', 'r', encoding='utf-8') as f:\n",
        "    ryz = f.read()\n",
        "\n",
        "text = \"\\\\n\\\\n\".join([okudjava, bedny, ancharov, gumilev, mandelshtam, nabokov, pasternak, losev, bar, ryz])  # Объединяем с разделителем"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvT1W7mhxzsJ",
        "outputId": "3ef9eb83-e4dc-4f79-91b4-1c00a497ae5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-22 21:07:48--  https://raw.githubusercontent.com/pltnv-a-a/nlp-notes/refs/heads/main/Boris_Pasternak.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 68528 (67K) [text/plain]\n",
            "Saving to: ‘Boris_Pasternak.txt’\n",
            "\n",
            "Boris_Pasternak.txt 100%[===================>]  66.92K  --.-KB/s    in 0.009s  \n",
            "\n",
            "2025-05-22 21:07:49 (6.90 MB/s) - ‘Boris_Pasternak.txt’ saved [68528/68528]\n",
            "\n",
            "--2025-05-22 21:07:49--  https://raw.githubusercontent.com/pltnv-a-a/nlp-notes/refs/heads/main/Lev_Losev.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 62041 (61K) [text/plain]\n",
            "Saving to: ‘Lev_Losev.txt’\n",
            "\n",
            "Lev_Losev.txt       100%[===================>]  60.59K  --.-KB/s    in 0.009s  \n",
            "\n",
            "2025-05-22 21:07:49 (6.72 MB/s) - ‘Lev_Losev.txt’ saved [62041/62041]\n",
            "\n",
            "--2025-05-22 21:07:49--  https://raw.githubusercontent.com/pltnv-a-a/nlp-notes/refs/heads/main/Evgeny_Baratynsky.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 40070 (39K) [text/plain]\n",
            "Saving to: ‘Evgeny_Baratynsky.txt’\n",
            "\n",
            "Evgeny_Baratynsky.t 100%[===================>]  39.13K  --.-KB/s    in 0.007s  \n",
            "\n",
            "2025-05-22 21:07:49 (5.33 MB/s) - ‘Evgeny_Baratynsky.txt’ saved [40070/40070]\n",
            "\n",
            "--2025-05-22 21:07:49--  https://raw.githubusercontent.com/pltnv-a-a/nlp-notes/refs/heads/main/Boris_Ryzhyi.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 45177 (44K) [text/plain]\n",
            "Saving to: ‘Boris_Ryzhyi.txt’\n",
            "\n",
            "Boris_Ryzhyi.txt    100%[===================>]  44.12K  --.-KB/s    in 0.008s  \n",
            "\n",
            "2025-05-22 21:07:50 (5.35 MB/s) - ‘Boris_Ryzhyi.txt’ saved [45177/45177]\n",
            "\n",
            "--2025-05-22 21:07:50--  https://raw.githubusercontent.com/pltnv-a-a/nlp-notes/refs/heads/main/Bulat_Okudjava.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 165476 (162K) [text/plain]\n",
            "Saving to: ‘Bulat_Okudjava.txt’\n",
            "\n",
            "Bulat_Okudjava.txt  100%[===================>] 161.60K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2025-05-22 21:07:50 (7.50 MB/s) - ‘Bulat_Okudjava.txt’ saved [165476/165476]\n",
            "\n",
            "--2025-05-22 21:07:50--  https://raw.githubusercontent.com/pltnv-a-a/nlp-notes/refs/heads/main/Demiyan_Bedny.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 311673 (304K) [text/plain]\n",
            "Saving to: ‘Demiyan_Bedny.txt’\n",
            "\n",
            "Demiyan_Bedny.txt   100%[===================>] 304.37K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2025-05-22 21:07:50 (10.8 MB/s) - ‘Demiyan_Bedny.txt’ saved [311673/311673]\n",
            "\n",
            "--2025-05-22 21:07:50--  https://raw.githubusercontent.com/pltnv-a-a/nlp-notes/refs/heads/main/Michail_Ancharov.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 73979 (72K) [text/plain]\n",
            "Saving to: ‘Michail_Ancharov.txt’\n",
            "\n",
            "Michail_Ancharov.tx 100%[===================>]  72.25K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2025-05-22 21:07:51 (4.79 MB/s) - ‘Michail_Ancharov.txt’ saved [73979/73979]\n",
            "\n",
            "--2025-05-22 21:07:51--  https://raw.githubusercontent.com/pltnv-a-a/nlp-notes/refs/heads/main/Nikolay_Gumilev.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 401324 (392K) [text/plain]\n",
            "Saving to: ‘Nikolay_Gumilev.txt’\n",
            "\n",
            "Nikolay_Gumilev.txt 100%[===================>] 391.92K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2025-05-22 21:07:51 (11.8 MB/s) - ‘Nikolay_Gumilev.txt’ saved [401324/401324]\n",
            "\n",
            "--2025-05-22 21:07:51--  https://raw.githubusercontent.com/pltnv-a-a/nlp-notes/refs/heads/main/Osip_Mandelshtam.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 459356 (449K) [text/plain]\n",
            "Saving to: ‘Osip_Mandelshtam.txt’\n",
            "\n",
            "Osip_Mandelshtam.tx 100%[===================>] 448.59K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2025-05-22 21:07:51 (14.2 MB/s) - ‘Osip_Mandelshtam.txt’ saved [459356/459356]\n",
            "\n",
            "--2025-05-22 21:07:51--  https://raw.githubusercontent.com/pltnv-a-a/nlp-notes/refs/heads/main/Vladimir_Nabokov.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 132328 (129K) [text/plain]\n",
            "Saving to: ‘Vladimir_Nabokov.txt’\n",
            "\n",
            "Vladimir_Nabokov.tx 100%[===================>] 129.23K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2025-05-22 21:07:52 (7.99 MB/s) - ‘Vladimir_Nabokov.txt’ saved [132328/132328]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "#batch_size = 16 # how many independent sequences will we process in parallel?\n",
        "#block_size = 32 # what is the maximum context length for predictions?\n",
        "#max_iters = 5000\n",
        "eval_interval = 100\n",
        "#learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "#n_embd = 64\n",
        "#n_head = 4\n",
        "#n_layer = 4\n",
        "#dropout = 0.0\n",
        "# ------------\n",
        "batch_size = 32      # Увеличенный размер батча\n",
        "block_size = 128     # Длина контекста, более подходящая для текста со строфами\n",
        "max_iters = 30000     # Больше итераций\n",
        "eval_interval = 200\n",
        "learning_rate = 3e-4 # Меняем скорость обучения\n",
        "n_embd = 128         # Увеличенная размерность эмбеддингов\n",
        "n_head = 8           # Больше голов внимания\n",
        "n_layer = 6          # Глубже архитектура\n",
        "dropout = 0.1        # Добавлена регуляризация\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.rope = RoPE(head_size, block_size)  # RoPE для каждого head\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # Применяем RoPE к query и key\n",
        "        q = self.rope(q)\n",
        "        k = self.rope(k)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    \"\"\"Улучшенный FFN с GELU активацией\"\"\"\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.GELU(),  # Более плавная активация\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, dim: int, eps: float = 1e-6):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.weight = nn.Parameter(torch.ones(dim))\n",
        "\n",
        "    def _norm(self, x):\n",
        "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.weight * self._norm(x)\n",
        "\n",
        "\n",
        "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n",
        "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
        "    t = torch.arange(end, device=freqs.device)\n",
        "    freqs = torch.outer(t, freqs)\n",
        "    return torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
        "\n",
        "class RoPE(nn.Module):\n",
        "    def __init__(self, dim: int, max_seq_len: int):\n",
        "        super().__init__()\n",
        "        self.freqs_cis = precompute_freqs_cis(dim, max_seq_len)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        seq_len = x.size(1)\n",
        "        freqs_cis = self.freqs_cis[:seq_len].view(1, seq_len, -1).to(x.device)\n",
        "        x_ = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))\n",
        "        x_out = torch.view_as_real(x_ * freqs_cis).flatten(-2)\n",
        "        return x_out.type_as(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedForward(n_embd)\n",
        "        self.ln1 = RMSNorm(n_embd)\n",
        "        self.ln2 = RMSNorm(n_embd)\n",
        "        self.alpha = 0.5  # residual scaling factor\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.alpha * self.sa(self.ln1(x))\n",
        "        x = x + self.alpha * self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.rope = RoPE(n_embd, block_size)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = RMSNorm(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "        # улучшенная инициализация\n",
        "        for module in self.modules():\n",
        "            if isinstance(module, nn.Linear):\n",
        "                nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "                if module.bias is not None:\n",
        "                    nn.init.zeros_(module.bias)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        tok_emb = self.token_embedding_table(idx)\n",
        "        x = self.rope(tok_emb)\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens, temperature=0.8, top_p=0.9):\n",
        "        \"\"\"Генерация с температурой и nucleus sampling\"\"\"\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            logits, _ = self(idx_cond)\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "\n",
        "            # Nucleus sampling\n",
        "            sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "            cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "            sorted_indices_to_remove = cumulative_probs > top_p\n",
        "            sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "            sorted_indices_to_remove[..., 0] = 0\n",
        "            indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
        "            logits[indices_to_remove] = -float('Inf')\n",
        "\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx\n",
        "\"\"\"\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\"\"\"\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDoLShS4-F74",
        "outputId": "aec068d3-4115-4773-e476-5475879422ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.225755 M parameters\n",
            "step 0: train loss 5.0726, val loss 5.0755\n",
            "step 200: train loss 2.7693, val loss 2.8425\n",
            "step 400: train loss 2.5695, val loss 2.6690\n",
            "step 600: train loss 2.4546, val loss 2.5735\n",
            "step 800: train loss 2.3880, val loss 2.5149\n",
            "step 1000: train loss 2.3365, val loss 2.4635\n",
            "step 1200: train loss 2.2949, val loss 2.4276\n",
            "step 1400: train loss 2.2628, val loss 2.4029\n",
            "step 1600: train loss 2.2259, val loss 2.3696\n",
            "step 1800: train loss 2.1917, val loss 2.3513\n",
            "step 2000: train loss 2.1624, val loss 2.3249\n",
            "step 2200: train loss 2.1370, val loss 2.2947\n",
            "step 2400: train loss 2.1027, val loss 2.2803\n",
            "step 2600: train loss 2.0819, val loss 2.2538\n",
            "step 2800: train loss 2.0607, val loss 2.2323\n",
            "step 3000: train loss 2.0390, val loss 2.2209\n",
            "step 3200: train loss 2.0135, val loss 2.1980\n",
            "step 3400: train loss 1.9928, val loss 2.1882\n",
            "step 3600: train loss 1.9760, val loss 2.1759\n",
            "step 3800: train loss 1.9599, val loss 2.1646\n",
            "step 4000: train loss 1.9322, val loss 2.1496\n",
            "step 4200: train loss 1.9240, val loss 2.1361\n",
            "step 4400: train loss 1.9042, val loss 2.1259\n",
            "step 4600: train loss 1.8856, val loss 2.1160\n",
            "step 4800: train loss 1.8738, val loss 2.1111\n",
            "step 5000: train loss 1.8568, val loss 2.0929\n",
            "step 5200: train loss 1.8446, val loss 2.0799\n",
            "step 5400: train loss 1.8327, val loss 2.0838\n",
            "step 5600: train loss 1.8193, val loss 2.0680\n",
            "step 5800: train loss 1.8078, val loss 2.0648\n",
            "step 6000: train loss 1.7955, val loss 2.0622\n",
            "step 6200: train loss 1.7855, val loss 2.0550\n",
            "step 6400: train loss 1.7749, val loss 2.0498\n",
            "step 6600: train loss 1.7659, val loss 2.0388\n",
            "step 6800: train loss 1.7564, val loss 2.0411\n",
            "step 7000: train loss 1.7477, val loss 2.0374\n",
            "step 7200: train loss 1.7410, val loss 2.0325\n",
            "step 7400: train loss 1.7313, val loss 2.0302\n",
            "step 7600: train loss 1.7229, val loss 2.0249\n",
            "step 7800: train loss 1.7140, val loss 2.0128\n",
            "step 7999: train loss 1.7060, val loss 2.0140\n",
            "\n",
            "Творить в рассетую по свете.\n",
            " \n",
            "1923\n",
            "\n",
            "\n",
            "Тайной, потельный и старой невесной\n",
            "На душных горят о мое ветрам.\n",
            "О простране нашей сердце скорею,\n",
            "Что мне пришли сердце медут.\n",
            " \n",
            "Ты узнаешь все великий мир,\n",
            "Старь душа сиганет и крутил.\n",
            "И много белого светят.\n",
            " \n",
            "Да вот скажется огни горела\n",
            "И своей позеленою рукава,\n",
            "Предней пустое и моя в двоей\n",
            "Во чужден другая с нам волны.\n",
            "\n",
            "Мне же уже было с небоскровых,\n",
            "\n",
            "Чтобы объявье наследних любом\n",
            "\n",
            "И не видеть на любовь скрепках.\n",
            "\n",
            " \n",
            "\n",
            "И в никогда не долгие, как на него голос.\n",
            "\n",
            "* * *\n",
            "\n",
            " \n",
            "\n",
            "Проздали, слепая кого–торожеской,\n",
            "\n",
            "светлой заразы по мне вечеров зародный.\n",
            "\n",
            "Века порого земля.\n",
            "\n",
            " \n",
            "\n",
            "Подошел в толпане и трудно открокол в дворе,\n",
            "\n",
            "полусом на тревожном реди,\n",
            "\n",
            "от негу твою из паданье,\n",
            "\n",
            "что ж готовиться рассеянье.\n",
            "\n",
            "И страшен стоить не спокойно с торжественный,\n",
            "\n",
            "про сниматься и простраший красный,\n",
            "\n",
            "там на полные наши из губы в трубкой,\n",
            "\n",
            "и покупившись в грусть первать.\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "И великой дурной страсти,\n",
            "\n",
            "что светлевый и молитвенный,\n",
            "\n",
            "и все того только трубая,\n",
            "\n",
            "и желто снова на без горячих в коротник.\n",
            "\n",
            " \n",
            "\n",
            "Позвучаю, не нет куладно красоты,\n",
            "\n",
            "и закател и в ночь полочной вечерой,\n",
            "\n",
            "почушен на укинь на дали мои,\n",
            "\n",
            "как смелой предо длинного далёки,\n",
            "\n",
            "в оскорбных других пленях совсех.\n",
            "\n",
            " \n",
            "\n",
            "И на пламенах безмеруют про рожа,\n",
            "\n",
            "и мы в полночный королям редь несравства.\n",
            "\n",
            " \n",
            "\n",
            "Я разумал 1918\n",
            "\n",
            "\n",
            "Из не верную землю поглубину...\n",
            "\n",
            "А один из пробатых главный листоков.\n",
            "\n",
            "И в коне замени скрипит из песенка\n",
            "\n",
            "одивал волновать и покричу по над мнем,\n",
            "\n",
            "как за ним свершеского мужинки\n",
            "\n",
            "и стоялись смертив не проходились,\n",
            "\n",
            "не нас, что там трогую страстно\n",
            "\n",
            "семи в топом дворенья светлы\n",
            "\n",
            "покрылая во мной кричатой помнит.\n",
            "\n",
            "\n",
            "* * *\n",
            "\n",
            " \n",
            "\n",
            "Мне воздух настраженых перед всегда\n",
            "\n",
            "совершен под сердь судьбы,\n",
            "\n",
            "светлы, за тростно и тонки,\n",
            "\n",
            "как не последняя стольки.\n",
            "\n",
            " \n",
            "\n",
            "Я душа со сейде страшный клятке,\n",
            "\n",
            "И размерных губы в там\n",
            "\n",
            "На пальсе трусованный колен.\n",
            "\n",
            " \n",
            "\n",
            "Не разнительной две сновой.\n",
            "\n",
            "И за какой из меня война люди,\n",
            "\n",
            "Как кольцам был советские должек.\n",
            "\n",
            " \n",
            "\n",
            "На роге, задыхая роская в нас,\n",
            "\n",
            "и с новетский ветра н\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = torch.tensor([encode(\"осень\")], dtype=torch.long, device=device)\n",
        "\n",
        "print(decode(m.generate(context, max_new_tokens=1000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LPPETdA8AOWN",
        "outputId": "6ef31d82-bd28-4e90-9c0e-70cefd485139"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "осенье -- моей общетил.\n",
            "\n",
            "Какая пенье больница моих кругой,\n",
            "\n",
            "не печали в утеполой лет, в руку, скрыться на сторах.\n",
            "\n",
            " \n",
            "\n",
            "А стоялся плотникой землей\n",
            "\n",
            "себя бы бесседний реской не надежность\n",
            "\n",
            "по выроскались и полосом и листи,\n",
            "\n",
            "И светло, над опортите проходят.\n",
            "\n",
            " \n",
            "\n",
            "Закатит звонков и слухи и веком\n",
            "\n",
            "смеют по бог отдавших мудой одной,\n",
            "\n",
            "то старин ложит небесет на ворона.\n",
            "\n",
            " \n",
            "\n",
            "Солнце видел не смотрит и верь,\n",
            "\n",
            "остроить верно сердца собачий,\n",
            "\n",
            "глубок неба ликолых славоров.\n",
            "\n",
            " \n",
            "\n",
            "Он слушались крошали тебя,\n",
            "\n",
            "в придумчивай строго ни в огне...\n",
            "\n",
            "В разнатительниц молодных лет,\n",
            "\n",
            "в все не жалость не один.\n",
            "\n",
            " \n",
            "\n",
            "На бледный торческий не прекостит –\n",
            "\n",
            "Поклянет всем другие, всеми,\n",
            "\n",
            "моя песня, ты в посмотренное из зверя.\n",
            "\n",
            " \n",
            "\n",
            "И в поднимах, как вечером постранничья,\n",
            "\n",
            "Привостовит на полоть новая лицо.\n",
            "\n",
            "И не тогда навернет туман.\n",
            "\n",
            "Вы детствая там, почтет и тепло на глубинки добротью.\n",
            " \n",
            "Ты могуче не стал, я то, скажу, что мне не убержествуять,\n",
            "Солдейшей ногу, о красной мой сприпней,\n",
            "И не детский, светлой дорогий будет,\n",
            "Чтобы нам \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = torch.tensor([encode(\"смерть\")], dtype=torch.long, device=device)\n",
        "\n",
        "print(decode(m.generate(context, max_new_tokens=1000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-OgjzkNaAPdz",
        "outputId": "3b5b1c1b-7793-43ea-81f1-ff77a29de1f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "смерть,\n",
            "    Как не за глубинами себя.\n",
            "    Повесть в комном колено плотница,\n",
            "        Одиного на мальчиком ты!\n",
            "Под страстно летела комнать оплоками ветер,\n",
            "Как замеркались куда из себя гордая,\n",
            "И не тобою плещалась и ручи толкому виновать,\n",
            "Как же мирости был за розов презраном глубом.\n",
            " \n",
            "И печалейского порогую и такома,\n",
            "Но роз в сокровавьев синой,\n",
            "И в землях мы на колоне другом,\n",
            "Что на выленнею поют.\n",
            " \n",
            "Все на мировою плиту,\n",
            "Минус голодные полоды,\n",
            "И войны городый трудный тебя звезды\n",
            "Тоже не не победа, в гулов своих.\n",
            "\n",
            " \n",
            "\n",
            "Прокойный рыни молодно всё не против кальцам,\n",
            "\n",
            "а на слезые губы на страну.\n",
            "\n",
            " \n",
            "\n",
            "Я прохожу я так не стройный труб\n",
            "\n",
            "Ничего опусть будут просторанский,\n",
            "\n",
            "Как причувшись, облоколистов,\n",
            "\n",
            "Как в плату силосьный строну.\n",
            "\n",
            " \n",
            "\n",
            "И будет ты больше пред мой темной повсе,\n",
            "\n",
            "на глядой сокрое поднимает.\n",
            "\n",
            "Последням на полном дому и с душой,\n",
            "\n",
            "Чтобы больше только детская возьм,\n",
            "\n",
            "От мне отошен смерть, с недострогит.\n",
            "\n",
            " \n",
            "\n",
            "Но со страшник наши огни.\n",
            "\n",
            "И к передставляется, в подворах.\n",
            "\n",
            "Твои почум непрослился, гр\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = torch.tensor([encode(\"на полустаночке\")], dtype=torch.long, device=device)\n",
        "\n",
        "print(decode(m.generate(context, max_new_tokens=1000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ygsBmQYrAUcO",
        "outputId": "b7ba54b8-fbc0-4854-e6ca-200ae4479437"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "на полустаночке в сиренье,\n",
            "\n",
            "     Сумранья в попусту.\n",
            "\n",
            " \n",
            "\n",
            "И в сердце после на воскресенной рощой,\n",
            "\n",
            "Как небеса и причем в другом прилетел,\n",
            "\n",
            "когда скажен не много почего пространной лица.\n",
            "\n",
            " \n",
            "\n",
            "Как не за земной человек небесые\n",
            "\n",
            "как поля бессмерти моей твоею печальной.\n",
            "\n",
            "О, как мы смерть и пристел маскал на терз.\n",
            "\n",
            "До крайм топочь с корме и медную темные крылый.\n",
            "\n",
            "Человечи взглянуть от все дворять в плене, —\n",
            "\n",
            "по в глубите, чтобы молчатной сеной,\n",
            "\n",
            "не продлесть следят в ночей барнит\n",
            "\n",
            "и за волосит не дивной прозрачный,\n",
            "\n",
            "что я проклятья, годит скажет за белой,\n",
            "\n",
            "от звенят он подобно, да народ,\n",
            "\n",
            "старинно и душистый струим плень,\n",
            "\n",
            "стереть остоит он нашим с радам.\n",
            "\n",
            "\n",
            "* * *\n",
            "\n",
            " \n",
            "\n",
            "Сборник «Оттор», а не настало –\n",
            "\n",
            "Да повсем, перевойска, страха.\n",
            "\n",
            "Как сказки дальнического раскрыть.\n",
            "\n",
            "Суравшись на стороже, был в кругласть смертный,\n",
            "\n",
            "и все было в береги наместно\n",
            "\n",
            "соборник солнце сердце до меня.\n",
            "\n",
            " \n",
            "\n",
            "А тому бы траве и одно на своды.\n",
            "\n",
            "Перевет, и постоим моим и немного рассе.\n",
            "\n",
            "Концовым взглянуть сталось западка в глубом,\n",
            "\n",
            "И моя про\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = torch.tensor([encode(\"лего\")], dtype=torch.long, device=device)\n",
        "\n",
        "print(decode(m.generate(context, max_new_tokens=1000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kt6NM8lCAeQT",
        "outputId": "79ce23bb-9970-466f-c6ef-46c05a3d8d21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "лего,\n",
            "\n",
            "с протеческой постельной землю,\n",
            "\n",
            "от войной страшный телеф.\n",
            "\n",
            " \n",
            "\n",
            "Не дышать с дроги попальный,\n",
            "\n",
            "как по море землю и россился.\n",
            "\n",
            "Для трепещет он мной тенаря,\n",
            "\n",
            "Как веки нежный улиц матрарей,\n",
            "\n",
            "То без руку ночи след грозят,\n",
            "\n",
            "Подобный снов в моей на свою,\n",
            "\n",
            "От первой люди, постретит\n",
            "\n",
            "Морские груди подрого кровь.\n",
            "\n",
            " \n",
            "\n",
            "Что вылетит без полночь\n",
            "\n",
            "Простих простав в блесне провет,\n",
            "\n",
            "что он простит дырамим капирым.\n",
            "\n",
            " \n",
            "\n",
            "Напрасным нам мируства вашей столен.\n",
            "\n",
            "То, что в родной взор странах земной,\n",
            "\n",
            "Как знающей дари каждый облак,\n",
            "\n",
            "На стране строй колыбкою друзей.\n",
            "\n",
            " \n",
            "\n",
            "Мой на двух над годы, дума,\n",
            "\n",
            "Темный от тосках и он до дела,\n",
            "\n",
            "Только открывает ликом отторгий,\n",
            "\n",
            "И очень струны и деревья, разве дрожь\n",
            "\n",
            "На посмотре верят в по тонких отвертил\n",
            "\n",
            "Белого на серебето вечером,\n",
            "\n",
            "Так с вороту на скальной и день.\n",
            "\n",
            " \n",
            "\n",
            "И так душе точно нежнее,\n",
            "\n",
            "А детские жарики, всему открытый,\n",
            "\n",
            "Держащим только струнит торопе.\n",
            "\n",
            " \n",
            "\n",
            "Ты виносим снами тебя солнце поворно снила,\n",
            "\n",
            "я скрипет дорогу он не пропит.\n",
            "\n",
            "Все потомных не привожно роз мне з\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = torch.tensor([encode(\"ты\")], dtype=torch.long, device=device)\n",
        "\n",
        "print(decode(m.generate(context, max_new_tokens=1000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NvVRermAiru",
        "outputId": "932f9b9f-fcb7-45f8-d776-7dbab4325b1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ты,\n",
            "И, кто странным волнам переделом,\n",
            "Прикрасным кардам в плеском продарим.\n",
            " \n",
            "Помню по королам и прекрасно и зловещи.\n",
            "На лицейских лила своегостной народилась кронит.\n",
            " \n",
            "И оболюбленных камнев и от века\n",
            "От разговолоси за левочется и дрожавает.\n",
            "Так в старицу простил с моря спокойный белой,\n",
            "Старый огоньк правдольный новостью раздора,\n",
            "Чужали рукованный на медной звуки.\n",
            " \n",
            "Песный луны нарена должной такий земле,\n",
            "Что заботы рабочей смерти один,\n",
            "Что все страшно по вырезу запачат вас,\n",
            "Мы ли там в совест без деле,\n",
            "И по небе приторного неувидел,\n",
            "И солнце соловились поролится.\n",
            " \n",
            "Автомоким душом распредины\n",
            "Наш облаконе отдевши в черной.\n",
            "В сердце на потом дрожалось,\n",
            "Чтоб перед пахается нам горячей.\n",
            " \n",
            "   Как полним в виденного на мертвый душат.\n",
            " \n",
            "И не мне смея взорить мира с окниграм,\n",
            "Велеется по ты страшишь не в обоим.\n",
            " \n",
            "И слезаешь другие окно, полуная беда!\n",
            "Только же заветала и сказали и они неба,\n",
            "Солнения, покина смотрел наша с темной,\n",
            "И полнен божать, как моря, как нас,\n",
            "И поднимается простил в снову\n"
          ]
        }
      ]
    }
  ]
}